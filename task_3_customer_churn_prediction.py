# -*- coding: utf-8 -*-
"""Task_3_Customer Churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139ia__0OAm0jlXidHz0OQXnAgGriF03o

# Task 3 : Customer Churn Prediction

Dependencies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

import xgboost as xgb
import shap

import warnings
warnings.filterwarnings("ignore")

"""# OBJECTIVE

my objective is to Predict which customers are likely to leave the bank using machine learning. This is a classic binary classification problem.

DATA SOURCE Platform: Kaggle This dataset contains the personal and financial details of 10,000 bank customers. The goal is to use these features to predict whether a customer will exit (leave) or stay.

**Exploratory Analysis**

Read the CSV and Perform Basic Data Cleaning
"""

from google.colab import files
uploaded = files.upload()

# Load the dataset
df = pd.read_csv("Churn_Modelling.csv")

#first few row of dataset
print("First few rows of the dataset:")
print(df.head())

df.info()

print(f"Dataframe dimensions: {df.shape}")
df.duplicated().sum()

""" Remove unnecessary identifier columns"""

df.drop(columns=["RowNumber", "CustomerId", "Surname"], inplace=True)
print(f"Dataframe dimensions: {df.shape}")
df.head()

df.describe()

# Summarize categorical features
df.describe(include=['object'])

"""# Encode Categorical Features

this data set have cotogorical columns: Geography, Gender

Geography have 3 cataogories (France, Spain, Germany), One-Hot Encoding works well here

Gender has 2 categories (Male, Female), Label Encoding is sufficient
"""

# Encoding Gender with LabelEncoder (Male=1, Female=0)
le_gender = LabelEncoder()
df['Gender'] = le_gender.fit_transform(df['Gender'])

# One-Hot Encode Geography, drop first to avoid dummy variable trap
df = pd.get_dummies(df, columns=['Geography'], drop_first=True)

"""# Separate dataframe into separate object Features and Target"""

# Object for target variable
y = df.Exited

# object for input features
X = df.drop(['Exited'], axis=1)

# display shapes of X and y
print(X.shape, y.shape)

#train test split
random_state = 10
# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=random_state,
                                                   stratify=df.Exited)
print(len(X_train), len(X_test), len(y_train), len(y_test))

"""# Model Training with XGBoost"""

model = xgb.XGBClassifier(
    eval_metric='logloss',
    colsample_bytree=0.8,
    learning_rate=0.1,
    max_depth=5,
    n_estimators=100,
    random_state=42
)


model.fit(X_train, y_train)

"""# prediction"""

y_pred = model.predict(X_test)

print("\n Classification Report:")
print(classification_report(y_test, y_pred))

print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}")

"""# PAIRPLOT"""

# Choose meaningful numerical features to avoid clutter
plot_df = df[['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'Exited']].copy()

# Seaborn expects hue column to have class labels for cleaner legend
plot_df['Exited'] = plot_df['Exited'].map({0: 0, 1: 1})  # keep numeric for plot
plot_df['Exited_label'] = plot_df['Exited'].map({0: 'Stays', 1: 'Exits'})

g = sns.pairplot(plot_df.drop('Exited', axis=1),  # drop numeric, use label instead
                 hue='Exited_label',
                 palette={ 'Exits' : 'green', 'Stays' : 'red' },
                 plot_kws={ 'alpha' : 0.8, 'edgecolor' : 'b', 'linewidth' : 0.5 })

# Adjust spacing and title
fig = g.fig
fig.subplots_adjust(top=0.95, wspace=0.2)
fig.suptitle('Plot by "Exited" Classes',
             fontsize=26,
             fontweight='bold')

# Update the legend
new_title = 'Churn Risk'
g._legend.set_title(new_title)

# Replace legend labels
new_labels = ['Stays', 'Exits']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

plt.show()

"""# confusion matrix"""

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Stayed', 'Exited'], yticklabels=['Stayed', 'Exited'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# CONCLUSION
The Churn Modelling dataset was used to build a binary classification model to predict whether a bank customer would exit. After cleaning and encoding the data

**XGBoost classifier**

I trained XGB BOOST which obtain the accuracy of approximately 86%

the model performed well overall, especially in identifying customers likely to stay

Visual analysis using a customized pairplot revealed that features like age and account balance had strong associations with churn. This model can help the bank identify at-risk customers and take proactive measures to improve retention
"""